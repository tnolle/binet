{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from multiprocessing.pool import Pool\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sqlalchemy.orm import Session\n",
    "from tqdm import tqdm\n",
    "\n",
    "from april.anomalydetection import BINet\n",
    "from april.anomalydetection.utils import label_collapse\n",
    "from april.database import Evaluation\n",
    "from april.database import Model\n",
    "from april.database import get_engine\n",
    "from april.enums import Base\n",
    "from april.enums import Heuristic\n",
    "from april.enums import Strategy\n",
    "from april.evaluator import Evaluator\n",
    "from april.fs import get_model_files\n",
    "from april.fs import PLOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristics = [h for h in Heuristic.keys() if h not in [Heuristic.DEFAULT, Heuristic.MANUAL, Heuristic.RATIO,\n",
    "                                                       Heuristic.MEDIAN, Heuristic.MEAN]]\n",
    "params = [(Base.SCORES, Heuristic.DEFAULT, Strategy.SINGLE), *itertools.product([Base.SCORES], heuristics, Strategy.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(params):\n",
    "    e, base, heuristic, strategy = params\n",
    "\n",
    "    session = Session(get_engine())\n",
    "    model = session.query(Model).filter_by(file_name=e.model_file.name).first()\n",
    "    session.close()\n",
    "\n",
    "    # Generate evaluation frames\n",
    "    y_pred = e.binarizer.binarize(base=base, heuristic=heuristic, strategy=strategy, go_backwards=False)\n",
    "    y_true = e.binarizer.get_targets()\n",
    "\n",
    "    evaluations = []\n",
    "    for axis in [0, 1, 2]:\n",
    "        for i, attribute_name in enumerate(e.dataset.attribute_keys):\n",
    "            def get_evaluation(label, precision, recall, f1):\n",
    "                return Evaluation(model_id=model.id, file_name=model.file_name,\n",
    "                                  label=label, perspective=perspective, attribute_name=attribute_name,\n",
    "                                  axis=axis, base=base, heuristic=heuristic, strategy=strategy,\n",
    "                                  precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "            perspective = 'Control Flow' if i == 0 else 'Data'\n",
    "            if i > 0 and not e.ad_.supports_attributes:\n",
    "                evaluations.append(get_evaluation('Normal', 0.0, 0.0, 0.0))\n",
    "                evaluations.append(get_evaluation('Anomaly', 0.0, 0.0, 0.0))\n",
    "            else:\n",
    "                yp = label_collapse(y_pred[:, :, i:i + 1], axis=axis).compressed()\n",
    "                yt = label_collapse(y_true[:, :, i:i + 1], axis=axis).compressed()\n",
    "                p, r, f, _ = metrics.precision_recall_fscore_support(yt, yp, labels=[0, 1])\n",
    "                evaluations.append(get_evaluation('Normal', p[0], r[0], f[0]))\n",
    "                evaluations.append(get_evaluation('Anomaly', p[1], r[1], f[1]))\n",
    "\n",
    "    return evaluations\n",
    "\n",
    "def evaluate(model_name):\n",
    "    e = Evaluator(model_name)\n",
    "\n",
    "    _params = []\n",
    "    for base, heuristic, strategy in params:\n",
    "        if e.dataset.num_attributes == 1 and strategy in [Strategy.ATTRIBUTE, Strategy.POSITION_ATTRIBUTE]:\n",
    "            continue\n",
    "        if isinstance(e.ad_, BINet) and e.ad_.version == 0:\n",
    "            continue\n",
    "        if heuristic is not None and heuristic not in e.ad_.supported_heuristics:\n",
    "            continue\n",
    "        if strategy is not None and strategy not in e.ad_.supported_strategies:\n",
    "            continue\n",
    "        if base is not None and base not in e.ad_.supported_bases:\n",
    "            continue\n",
    "        _params.append([e, base, heuristic, strategy])\n",
    "\n",
    "    return [_e for p in _params for _e in _evaluate(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 48/48 [01:00<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "models = sorted([m.name for m in get_model_files() if m.p == 0.3 and 'real' in m.name])\n",
    "\n",
    "evaluations = []\n",
    "with Pool() as p:\n",
    "    for e in tqdm(p.imap(evaluate, models), total=len(models), desc='Evaluate'):\n",
    "        evaluations.append(e)\n",
    "\n",
    "# Write to database\n",
    "session = Session(get_engine())\n",
    "for e in evaluations:\n",
    "    session.bulk_save_objects(e)\n",
    "    session.commit()\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 317304/317304 [00:05<00:00, 60654.97it/s]\n"
     ]
    }
   ],
   "source": [
    "out_dir = PLOT_DIR / 'isj-2019'\n",
    "eval_file = out_dir / 'eval.pkl'\n",
    "\n",
    "session = Session(get_engine())\n",
    "evaluations = session.query(Evaluation).all()\n",
    "rows = []\n",
    "for ev in tqdm(evaluations):\n",
    "    m = ev.model\n",
    "    el = ev.model.training_event_log\n",
    "    rows.append([m.file_name, m.creation_date, m.hyperparameters, m.training_duration, m.training_host, m.algorithm, \n",
    "                 el.name, el.base_name, el.percent_anomalies, el.number,\n",
    "                 ev.axis, ev.base, ev.heuristic, ev.strategy, ev.label, ev.attribute_name, ev.perspective, ev.precision, ev.recall, ev.f1])\n",
    "session.close()\n",
    "columns = ['file_name', 'date', 'hyperparameters', 'training_duration', 'training_host', 'ad',\n",
    "           'dataset_name', 'process_model', 'noise', 'dataset_id',\n",
    "           'axis', 'base', 'heuristic', 'strategy', 'label', 'attribute_name', 'perspective', 'precision', 'recall', 'f1']\n",
    "evaluation = pd.DataFrame(rows, columns=columns)\n",
    "evaluation.to_pickle(eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
